# -*- coding: utf-8 -*-
"""Dimensionality Reduction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17lTzCCO1wN6C9UCKAkpvjRRjNUg_bz73
"""

import requests
from bs4 import BeautifulSoup
import zipfile
import io
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, classification_report
import numpy as np
import time

# Function to download and load dataset
def load_data():
    page_url = 'https://archive.ics.uci.edu/dataset/240/human+activity+recognition+using+smartphones'
    page_response = requests.get(page_url)
    if page_response.status_code == 200:
        soup = BeautifulSoup(page_response.content, 'html.parser')
        download_link = soup.select_one('a[href$=".zip"]')['href']
        full_download_url = 'https://archive.ics.uci.edu' + download_link
        response = requests.get(full_download_url)
        if response.status_code == 200:
            with zipfile.ZipFile(io.BytesIO(response.content)) as outer_zip:
                inner_zip_name = 'UCI HAR Dataset.zip'
                with outer_zip.open(inner_zip_name) as inner_zip_file:
                    with zipfile.ZipFile(io.BytesIO(inner_zip_file.read())) as inner_zip:
                        with inner_zip.open('UCI HAR Dataset/train/X_train.txt') as myfile:
                            df = pd.read_csv(myfile, delim_whitespace=True, header=None)
                        with inner_zip.open('UCI HAR Dataset/train/y_train.txt') as myfile_y:
                            y = pd.read_csv(myfile_y, delim_whitespace=True, header=None)
    else:
        raise Exception("Failed to download or parse the dataset.")
    return df, y

# Load dataset
df, y = load_data()

#TASK 1 - DO EDA and understand a little about the data.
#Only important thing is to know that it has a lot of features that don't make sense, just a
#bunch of readings from sensors.
#We think many of these features are redundant or irrelevant, and we want to find good features.

# Display the first few rows of the dataset
print(df.head())

# Get the shape of the dataset (number of rows and columns)
print(df.shape)

# Get the data types of each column
print(df.dtypes)

# Check for missing values
print(df.isnull().sum())

# Get summary statistics for numerical features
print(df.describe())

# Task 2: Encode class labels
from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()
encoded_y = label_encoder.fit_transform(y.values.ravel())

# Task 3: Scale the features using StandardScaler
# YOUR CODE HERE: Apply StandardScaler to df
# Import LabelEncoder
from sklearn.preprocessing import LabelEncoder

# Create a LabelEncoder object
label_encoder = LabelEncoder()

# Fit the encoder to the target variable (y) and transform it
encoded_y = label_encoder.fit_transform(y)

# Task 4: Split the data into training and testing sets
# YOUR CODE HERE: Use train_test_split to split the data
from sklearn.model_selection import train_test_split

X_train_full, X_test_full, y_train, y_test = train_test_split(
    df, encoded_y, test_size=0.2, random_state=42
)

# Task 5:
# 1. Create a pipeline using Gaussian Naive Bayes
from sklearn.naive_bayes import GaussianNB
pipeline = Pipeline([
    ('classifier', GaussianNB())
])

# 2. Fit the model to the training data
pipeline.fit(X_train_full, y_train.ravel())  # Corrected line

# 3. Predict values for the test set
y_pred = pipeline.predict(X_test_full)

# 4. Print accuracy score
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy Score: {accuracy:.4f}")

import time

# 1. Note the start time before defining the pipeline
start_time = time.time()

# Define and fit the pipeline using Gaussian Naive Bayes
pipeline = Pipeline([
    ('classifier', GaussianNB())
])
pipeline.fit(X_train_full, y_train.ravel())  # Corrected line

# Predict values for the test set
y_pred = pipeline.predict(X_test_full)

# 2. Note the end time and calculate the difference
end_time = time.time()
time_taken = end_time - start_time

# Print the time taken
print(f"Time taken for model training and inference: {time_taken:.4f} seconds")

# TASK 7 - K-Means for dimensionality reduction
n_clusters = 50  # Choose the desired number of clusters
kmeans = KMeans(n_clusters=n_clusters, random_state=42)  # Initialize KMeans
kmeans.fit(df.T)  # Transpose to treat features as data points
selected_features_indices = []
for i in range(n_clusters):
    cluster_indices = np.where(kmeans.labels_ == i)[0]
    selected_features_indices.append(cluster_indices[0])  # Select one feature per cluster
selected_features = df.iloc[:, selected_features_indices]  # Extract the selected features

from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score
import time

# Split the selected features into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    selected_features, encoded_y, test_size=0.2, random_state=42
)

# Train the GaussianNB model
start_time = time.time()
gnb_model = GaussianNB()
gnb_model.fit(X_train, y_train)
end_time = time.time()

# Make predictions
y_pred = gnb_model.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)

# Print the results
print("Time taken to train:", end_time - start_time, "seconds")
print("Accuracy:", accuracy)